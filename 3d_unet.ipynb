{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n#%matplotlib inline\nimport tensorflow as tf\nimport keras.backend as K\nfrom keras.utils import to_categorical\nfrom keras import metrics\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, BatchNormalization, Activation, Dense, Dropout,Maximum\nfrom keras.layers.core import Lambda, RepeatVector, Reshape\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose,Conv3D,Conv3DTranspose\nfrom keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D,MaxPooling3D\nfrom keras.layers.merge import concatenate, add\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n\nfrom skimage.io import imread, imshow, concatenate_images\nfrom skimage.transform import resize\nfrom sklearn.utils import class_weight\n\n\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import CSVLogger\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers.advanced_activations import PReLU\n\nimport os\nfrom skimage.io import imread, imshow, concatenate_images\nfrom skimage.transform import resize\n# from medpy.io import load\nimport numpy as np\n\n#import cv2\nimport nibabel as nib\nfrom PIL import Image\n\n\n\ndef conv_block(input_mat,num_filters,kernel_size,batch_norm):\n  X = Conv3D(num_filters,kernel_size=(kernel_size,kernel_size,kernel_size),strides=(1,1,1),padding='same')(input_mat)\n  if batch_norm:\n    X = BatchNormalization()(X)\n  \n  X = Activation('relu')(X)\n\n  X = Conv3D(num_filters,kernel_size=(kernel_size,kernel_size,kernel_size),strides=(1,1,1),padding='same')(X)\n  if batch_norm:\n    X = BatchNormalization()(X)\n  \n  X = Activation('relu')(X)\n    \n  X = add([input_mat,X]);\n  \n  return X\n\n\ndef Vnet_3d(input_img, n_filters = 8, dropout = 0.2, batch_norm = True):\n\n  #c1 = conv_block(input_img,n_filters,3,batch_norm)\n  c1 = Conv3D(n_filters,kernel_size = (5,5,5) , strides = (1,1,1) , padding='same')(input_img)\n  #c1 = add([c1,input_img])\n  \n  c2 = Conv3D(n_filters*2,kernel_size = (2,2,2) , strides = (2,2,2) , padding = 'same' )(c1)\n  \n  c3 = conv_block(c2 , n_filters*2,5,True)\n  \n  p3 = Conv3D(n_filters*4,kernel_size = (2,2,2) , strides = (2,2,2), padding = 'same')(c3)\n  p3 = Dropout(dropout)(p3)\n  \n  c4 = conv_block(p3, n_filters*4,5,True)\n  p4 = Conv3D(n_filters*8,kernel_size = (2,2,2) , strides = (2,2,2) , padding='same')(c4)\n  p4 = Dropout(dropout)(p4)\n    \n  c5 = conv_block(p4, n_filters*8,5,True)\n  p6 = Conv3D(n_filters*16,kernel_size = (2,2,2) , strides = (2,2,2) , padding='same')(c5)\n  p6 = Dropout(dropout)(p6)\n  #c6 = conv_block(p5, n_filters*8,5,True)\n  #p6 = Conv3D(n_filters*16,kernel_size = (2,2,2) , strides = (2,2,2) , padding='same')(c6)\n\n  p7 = conv_block(p6,n_filters*16,5,True)\n    \n  u6 = Conv3DTranspose(n_filters*8, (2,2,2), strides=(2, 2, 2), padding='same')(p7);\n  u6 = concatenate([u6,c5]);\n  c7 = conv_block(u6,n_filters*16,5,True)\n  c7 = Dropout(dropout)(c7)\n  u7 = Conv3DTranspose(n_filters*4,(2,2,2),strides = (2,2,2) , padding= 'same')(c7);\n\n  \n  u8 = concatenate([u7,c4]);\n  c8 = conv_block(u8,n_filters*8,5,True)\n  c8 = Dropout(dropout)(c8)\n  u9 = Conv3DTranspose(n_filters*2,(2,2,2),strides = (2,2,2) , padding= 'same')(c8);\n    \n  u9 = concatenate([u9,c3]);\n  c9 = conv_block(u9,n_filters*4,5,True)\n  c9 = Dropout(dropout)(c9)\n  u10 = Conv3DTranspose(n_filters,(2,2,2),strides = (2,2,2) , padding= 'same')(c9);\n  \n  \n  u10 = concatenate([u10,c1]);\n  c10 = Conv3D(n_filters*2,kernel_size = (5,5,5),strides = (1,1,1) , padding = 'same')(u10);\n  c10 = Dropout(dropout)(c10)\n  c10 = add([c10,u10]);\n  \n  \n\n  #c9 = conv_block(u9,n_filters,3,batch_norm)\n  outputs = Conv3D(4, (1,1,1), activation='softmax')(c10)\n\n  model = Model(inputs=input_img, outputs=outputs)\n\n  return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dice_coef(y_true, y_pred, epsilon=0.00001):\n    \"\"\"\n    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n    \n    \"\"\"\n    axis = (0,1,2,3)\n    dice_numerator = 2. * K.sum(y_true * y_pred, axis=axis) + epsilon\n    dice_denominator = K.sum(y_true*y_true, axis=axis) + K.sum(y_pred*y_pred, axis=axis) + epsilon\n    return K.mean((dice_numerator)/(dice_denominator))\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1-dice_coef(y_true, y_pred)\n\ndef standardize(image):\n\n  standardized_image = np.zeros(image.shape)\n\n  #\n  \n      # iterate over the `z` dimension\n  for z in range(image.shape[2]):\n      # get a slice of the image \n      # at channel c and z-th dimension `z`\n      image_slice = image[:,:,z]\n\n      # subtract the mean from image_slice\n      centered = image_slice - np.mean(image_slice)\n      \n      # divide by the standard deviation (only if it is different from zero)\n      if(np.std(centered)!=0):\n          centered = centered/np.std(centered) \n\n      # update  the slice of standardized image\n      # with the scaled centered and scaled image\n      standardized_image[:, :, z] = centered\n\n  ### END CODE HERE ###\n\n  return standardized_image\n\n\ninput_img = Input((128,128,128,4))\nmodel = Vnet_3d(input_img,8,0.15,True)\nlearning_rate = 0.00095\n#epochs = 5000\ndecay_rate = 0.0000002\nmodel.compile(optimizer=Adam(lr=learning_rate, decay = decay_rate), loss=dice_coef_loss, metrics=[dice_coef])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = '../input/vs-brats2018/miccai_brats_2018_data_training/HGG'\nall_images = os.listdir(path)\n#print(len(all_images))\nall_images.sort()\ndata = np.zeros((240,240,155,4))\nimage_data2=np.zeros((240,240,155))\nloss_hist = []\naccu_hist = []\nepoch_wise_loss = []\nepoch_wise_accu = []\nfor epochs in range(50):\n  epoch_loss = 0\n  epoch_accu = 0\n  for image_num in range(180):\n    print(epochs)\n    print(image_num)\n\n# data preprocessing starts here\n\n    x = all_images[image_num]\n    print(x)\n    folder_path = path + '/' + x;\n    modalities = os.listdir(folder_path)\n    modalities.sort()\n    #data = []\n    w = 0\n    for j in range(len(modalities)):\n      #print(modalities[j])\n      \n      image_path = folder_path + '/' + modalities[j]\n      if not(image_path.find('seg.nii') == -1):\n        img = nib.load(image_path);\n        image_data2 = img.get_data()\n        image_data2 = np.asarray(image_data2)\n        print(\"Entered ground truth\")\n      else:\n        img = nib.load(image_path);\n        image_data = img.get_data()\n        image_data = np.asarray(image_data)\n        image_data = standardize(image_data)\n        data[:,:,:,w] = image_data\n        print(\"Entered modality\")\n        w = w+1\n      \n    print(data.shape)\n    print(image_data2.shape)  \n\n    reshaped_data=data[56:184,75:203,13:141,:]\n    reshaped_data=reshaped_data.reshape(1,128,128,128,4)\n    reshaped_image_data2=image_data2[56:184,75:203,13:141]\n\n        \n    reshaped_image_data2=reshaped_image_data2.reshape(1,128,128,128)\n    reshaped_image_data2[reshaped_image_data2==4] = 3\n    hello = reshaped_image_data2.flatten()\n    #y_to = keras.utils.to_categorical(y_to,num_classes=2)\n    print(reshaped_image_data2.shape)\n    #print(hello[hello==3].shape)\n    print(\"Number of classes\",np.unique(hello))\n    class_weights = class_weight.compute_class_weight('balanced',np.unique(hello),hello)\n    print(class_weights)\n\n    reshaped_image_data2 = to_categorical(reshaped_image_data2, num_classes = 4)\n\n    print(reshaped_data.shape)\n    print(reshaped_image_data2.shape)\n    print(type(reshaped_data))\n\n\n    history = model.fit(x=reshaped_data,y=reshaped_image_data2, epochs = 1 , class_weight = class_weights)\n    print(history.history['loss'])\n    epoch_loss += history.history['loss'][0]\n    epoch_accu += history.history['dice_coef'][0]\n    \n    loss_hist.append(history.history['loss'])\n    accu_hist.append(history.history['dice_coef'])\n  \n  model.save('../working/Vnet_model.h5')\n  epoch_loss = epoch_loss/180\n  epoch_accu = epoch_accu/180\n\n  epoch_wise_loss.append(epoch_loss)\n  epoch_wise_accu.append(epoch_accu)\n  \n  plt.plot(epoch_wise_loss)\n  plt.title('Model_loss vs epochs')\n  plt.ylabel('Loss')\n  plt.xlabel('epochs')\n  s = '../working/epochwise_loss_' + str(epochs)\n  plt.savefig(s)\n  plt.show()\n  plt.close()\n  \n  plt.plot(epoch_wise_accu)\n  plt.title('Model_Accuracy vs epochs')\n  plt.ylabel('Accuracy')\n  plt.xlabel('epochs')\n  s = '../working/epochwise_accu_' + str(epochs)\n  plt.savefig(s)\n  plt.show()\n  plt.close()\n    \n  plt.plot(accu_hist)\n  plt.title('model accuracy')\n  plt.ylabel('accuracy')\n  plt.xlabel('epoch')\n  s = '../working/accuracy_plot_' + str(epochs)\n  plt.savefig(s)\n  plt.show()\n  plt.close()\n    \n  plt.plot(loss_hist)\n  plt.title('model loss')\n  plt.ylabel('loss')\n  plt.xlabel('epoch')\n  s = '../working/loss_plot_' + str(epochs)\n  plt.savefig(s)\n  plt.show()\n  plt.close()\n\nmodel.save('../working/Vnet_model_4.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}